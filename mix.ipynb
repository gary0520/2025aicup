{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5546194c",
   "metadata": {},
   "source": [
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe89207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料已成功合併並儲存至 /home/student1/ai/train3/task1_answer.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 讀取 train1 和 train2 的檔案\n",
    "train1_path = '/home/student1/ai/train1/task1_answer.txt'\n",
    "train2_path = '/home/student1/ai/train2/task1_answer.txt'\n",
    "output_path = '/home/student1/ai/train3/task1_answer.txt'\n",
    "\n",
    "# 讀取 train1 資料\n",
    "with open(train1_path, 'r', encoding='utf-8') as file1:\n",
    "    train1_data = [line.strip() for line in file1.readlines()]\n",
    "\n",
    "# 讀取 train2 資料並過濾 fid > 80000 的資料\n",
    "with open(train2_path, 'r', encoding='utf-8') as file2:\n",
    "    train2_data = []\n",
    "    for line in file2.readlines():\n",
    "        parts = line.strip().split('\\t')\n",
    "        fid = int(parts[0])  # fid 是資料的第一欄\n",
    "        if fid <= 80000:\n",
    "            train2_data.append(line.strip())\n",
    "\n",
    "# 合併資料\n",
    "merged_data = train1_data + train2_data\n",
    "\n",
    "# 寫入合併後的資料到 train3 資料夾\n",
    "with open(output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    writer = csv.writer(output_file, delimiter='\\t')\n",
    "    for line in merged_data:\n",
    "        writer.writerow(line.split('\\t'))\n",
    "\n",
    "print(f\"資料已成功合併並儲存至 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ede16",
   "metadata": {},
   "source": [
    "answer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e21e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料已成功合併並儲存至 task1_answer.txt 和 task2_answer.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 讀取 train1 和 train2 的檔案\n",
    "task1_train1_path = '/home/student1/ai/train1/task1_answer.txt'\n",
    "task2_train1_path = '/home/student1/ai/train1/task2_answer.txt'\n",
    "task1_train2_path = '/home/student1/ai/train2/task1_answer.txt'\n",
    "task2_train2_path = '/home/student1/ai/train2/task2_answer.txt'\n",
    "\n",
    "task1_output_path = '/home/student1/ai/train3/task1_answer.txt'\n",
    "task2_output_path = '/home/student1/ai/train3/task2_answer.txt'\n",
    "\n",
    "# 讀取 task1 資料\n",
    "with open(task1_train1_path, 'r', encoding='utf-8') as file1:\n",
    "    task1_train1_data = [line.strip() for line in file1.readlines()]\n",
    "\n",
    "# 讀取 task2 資料\n",
    "with open(task2_train1_path, 'r', encoding='utf-8') as file2:\n",
    "    task2_train1_data = [line.strip() for line in file2.readlines()]\n",
    "\n",
    "# 讀取 task1 資料並過濾 fid > 80000 的資料\n",
    "with open(task1_train2_path, 'r', encoding='utf-8') as file2:\n",
    "    task1_train2_data = []\n",
    "    for line in file2.readlines():\n",
    "        parts = line.strip().split('\\t')\n",
    "        fid = int(parts[0])  # fid 是資料的第一欄\n",
    "        if fid <= 80000:\n",
    "            task1_train2_data.append(line.strip())\n",
    "\n",
    "# 讀取 task2 資料並過濾 fid > 80000 的資料\n",
    "with open(task2_train2_path, 'r', encoding='utf-8') as file2:\n",
    "    task2_train2_data = []\n",
    "    for line in file2.readlines():\n",
    "        parts = line.strip().split('\\t')\n",
    "        fid = int(parts[0])  # fid 是資料的第一欄\n",
    "        if fid <= 80000:\n",
    "            task2_train2_data.append(line.strip())\n",
    "\n",
    "# 合併資料\n",
    "merged_task1_data = task1_train1_data + task1_train2_data\n",
    "merged_task2_data = task2_train1_data + task2_train2_data\n",
    "\n",
    "# 寫入合併後的資料到 train3 資料夾的 task1_answer.txt 和 task2_answer.txt\n",
    "with open(task1_output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    writer = csv.writer(output_file, delimiter='\\t')\n",
    "    for line in merged_task1_data:\n",
    "        writer.writerow(line.split('\\t'))\n",
    "\n",
    "with open(task2_output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    writer = csv.writer(output_file, delimiter='\\t')\n",
    "    for line in merged_task2_data:\n",
    "        writer.writerow(line.split('\\t'))\n",
    "\n",
    "print(\"資料已成功合併並儲存至 task1_answer.txt 和 task2_answer.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0396685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合併完成，輸出檔案：/home/student1/ai/GPT/merged_phi_by_audio.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 來源資料夾：標準化過的 JSON 檔案\n",
    "standardized_folder = \"/home/student1/ai/GPT\"\n",
    "\n",
    "# 檔案清單（需確保這些檔案存在於上面資料夾）\n",
    "file_names = [\n",
    "    \"address_entities_by_line.json\",\n",
    "    \"dates_by_line.json\",\n",
    "    \"doctors_by_line.json\",\n",
    "    \"refined_hospitals_by_line.json\",\n",
    "    \"relaxed_id_number_by_line.json\",\n",
    "    \"relaxed_med_record_and_department_by_line.json\",\n",
    "    \"zip_code_by_line.json\"\n",
    "]\n",
    "\n",
    "# 合併結果（以音檔編號為 key）\n",
    "merged_by_audio = defaultdict(dict)\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(standardized_folder, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ 找不到檔案：{file_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for line_id, entity_dict in data.items():\n",
    "            # 擷取音檔前綴（例如 80001_01 → 80001）\n",
    "            audio_id = line_id.split(\"_\")[0] if \"_\" in line_id else line_id\n",
    "\n",
    "            for entity_type, values in entity_dict.items():\n",
    "                merged_by_audio[audio_id].setdefault(entity_type, []).extend(values)\n",
    "\n",
    "# 去重與排序\n",
    "for audio_id in merged_by_audio:\n",
    "    for entity_type in merged_by_audio[audio_id]:\n",
    "        merged_by_audio[audio_id][entity_type] = sorted(set(merged_by_audio[audio_id][entity_type]))\n",
    "\n",
    "# 輸出合併後的 JSON\n",
    "output_path = \"/home/student1/ai/GPT/merged_phi_by_audio.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict(merged_by_audio), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 合併完成，輸出檔案：{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
