{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafab12a",
   "metadata": {
    "id": "bafab12a"
   },
   "source": [
    "#### 安裝套件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f07a",
   "metadata": {
    "id": "9647f07a"
   },
   "source": [
    "#### 載入函式庫與參數設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e93b01",
   "metadata": {
    "executionInfo": {
     "elapsed": 26964,
     "status": "ok",
     "timestamp": 1748025980624,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "69e93b01"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import EarlyStoppingCallback\n",
    "from ctranslate2.converters import TransformersConverter\n",
    "import whisperx\n",
    "from transformers import WhisperProcessor, WhisperTokenizer\n",
    "import json\n",
    "import zipfile\n",
    "from jiwer import wer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58537a9a",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1748025980641,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "58537a9a"
   },
   "outputs": [],
   "source": [
    "def setup_directories(version: str = \"v99\"):\n",
    "    # Create result directory and version subdirectory\n",
    "    result_dir = \"model_result\"\n",
    "    result_dir = os.path.join(result_dir, \"task1\")\n",
    "    version_dir = os.path.join(result_dir, version)\n",
    "    model_dir = os.path.join(version_dir, \"model\")\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    os.makedirs(version_dir, exist_ok=True) \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(result_dir, version_dir, model_dir)\n",
    "    return version_dir, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c41f26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1748025982136,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "83c41f26",
    "outputId": "1a0468ec-b503-4b3a-dca0-952b75e7f355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_result/task1 model_result/task1/v1 model_result/task1/v1/model\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n",
      "Model saved to model_result/task1/v1/model/fine-tuning-whisper\n"
     ]
    }
   ],
   "source": [
    "# Setup directories\n",
    "version = \"v1\"\n",
    "version_dir, model_dir = setup_directories(version)\n",
    "\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 設定模型路徑\n",
    "final_model_path = os.path.join(model_dir, \"fine-tuning-whisper\")\n",
    "final_ct2_model_path = os.path.join(model_dir, \"fine-tuning-whisper-ct2\")\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "\n",
    "# load fine-tuning whisper\n",
    "zip_model_path = \"/home/student1/ai/fine-tuning-whisper.zip\"\n",
    "extract_model_path = \"/home/student1/model_result/task1/v1/model/fine-tuning-whisper\"\n",
    "with zipfile.ZipFile(zip_model_path, 'r') as zip_ref:\n",
    "  zip_ref.extractall(extract_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0de73c",
   "metadata": {
    "id": "8e0de73c"
   },
   "source": [
    "#### 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf1f09e",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1748025982186,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "adf1f09e"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(validation_path: str): # train_path: str,\n",
    "\n",
    "    val_audio_files = [f for f in os.listdir(validation_path) if f.endswith('.wav')]\n",
    "    val_dict = {\n",
    "        \"audio\": [os.path.join(validation_path, audio_file) for audio_file in val_audio_files],\n",
    "        \"text\": [\"\" for _ in val_audio_files]  # 空字串作為預設文字\n",
    "    }\n",
    "    val_dataset = Dataset.from_dict(val_dict)\n",
    "    val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    # 回傳兩個資料集\n",
    "    return {\n",
    "        \"val\": val_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57df62d7",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748025983426,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "57df62d7"
   },
   "outputs": [],
   "source": [
    "def prepare_features(batch, processor):\n",
    "    # Process audio\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-mel input features from input audio array\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Encode target text to label ids 並限制最大長度為 448\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=448,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "\n",
    "    ).input_ids\n",
    "    batch[\"file_path\"] = batch[\"audio\"][\"path\"]\n",
    "    batch[\"audio_file_name\"] = os.path.splitext(os.path.basename(batch[\"audio\"][\"path\"]))[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ae8b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "3f118e5edfe74538b2332d4edcee7cbe",
      "dda8b26af1f446f99a4c9f252883ffd6",
      "7529fd6f27c946a2a206c1085d755b8f",
      "29ee39fda62948d5809f7a27ccd6e10d",
      "85a6a3231f2e4b4a82fcde55803978e2",
      "382aa3bb596b4f7f9a7da828ae568b2e",
      "b96b87c377df49ecbe1f0e2e19090875",
      "c0dd9f90f828471abdb5eefd244f64f5",
      "c99ddc0ee4ae4b20966ea8cdc0a54d0d",
      "6ba43ffec427441fb9876820341c4c35",
      "0988dd35b617477087e7395004b0b2fc"
     ]
    },
    "executionInfo": {
     "elapsed": 20893,
     "status": "ok",
     "timestamp": 1748026010197,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "e8ae8b42",
    "outputId": "87b32fcc-0545-4343-c12f-7ff77b7a73ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d77a6bf638488daa27e068362ea810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "processor = WhisperProcessor.from_pretrained(\"/home/student1/ai/model_result/task1/v1/model/fine-tuning-whisper\")\n",
    "# \"audio_dataset/sample/Training_Dataset\",\n",
    "dataset = prepare_dataset(\"/home/student1/ai/Private_dataset/private\")\n",
    "# Process datasets\n",
    "processed_dataset = {}\n",
    "for split in [\"val\"]: # \"train\", \"test\",\n",
    "    print(f\"Processing {split} dataset...\")\n",
    "    processed_dataset[split] = dataset[split].map(\n",
    "        lambda x: prepare_features(x, processor=processor),\n",
    "        remove_columns= dataset[split].column_names,\n",
    "        num_proc=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335d695",
   "metadata": {
    "id": "6335d695"
   },
   "source": [
    "#### 進行轉檔 Huggingface to Ctranslate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4a87fe",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1748026017975,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "fa4a87fe"
   },
   "outputs": [],
   "source": [
    "def convert_hf_model_to_ct2(model_name_or_path: str, output_dir: str, quantization: str = \"float32\", trust_remote_code: bool = True, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Hugging Face convert to  CTranslate2\n",
    "\n",
    "    parameter:\n",
    "        model_name_or_path (str): Hugging Face moedel name or local directory.\n",
    "        output_dir (str): Output directory where the CTranslate2 model is saved.\n",
    "        quantization (str): Weight quantization scheme (possible values are: int8, int8_float32, int8_float16, int8_bfloat16, int16, float16, bfloat16, float32).\n",
    "        trust_remote_code (bool): Allow converting models using custom code.\n",
    "    \"\"\"\n",
    "    # 初始化轉換器\n",
    "    converter = TransformersConverter(model_name_or_path,copy_files=[\"preprocessor_config.json\", \"tokenizer.json\"] , trust_remote_code=trust_remote_code)\n",
    "    # 執行轉換\n",
    "    converter.convert(output_dir=output_dir,force=True)\n",
    "    model = whisperx.load_model(output_dir, device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5cb10a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7527,
     "status": "ok",
     "timestamp": 1748026027181,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "5c5cb10a",
    "outputId": "cd5261d6-f1ef-40bf-e28e-de7bb23d6432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model to CTranslate2 format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f778dfde8754fbf980a36d6a770c04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../miniconda3/envs/lin/lib/python3.11/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting model to CTranslate2 format...\")\n",
    "model = convert_hf_model_to_ct2(\n",
    "    model_name_or_path=final_model_path,\n",
    "    output_dir=final_ct2_model_path,\n",
    "    # quantization=\"float16\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d13916",
   "metadata": {
    "id": "b0d13916"
   },
   "source": [
    "#### WhisperX進行轉錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb248a40",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1748026036673,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "bb248a40"
   },
   "outputs": [],
   "source": [
    "# Save predictions to TSV file  VALIDATION\n",
    "def save_predictions_to_tsv(predictions,filenames, version_dir,export_json_file):\n",
    "    task1_output_file = os.path.join(version_dir, \"task1_answer.txt\")\n",
    "    output_file = os.path.join(version_dir, \"val_results.txt\")\n",
    "    print(\"Saving predictions to file...\")\n",
    "    json_output_file = os.path.join(version_dir, \"val_time_step.json\")\n",
    "    with open(json_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export_json_file, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Results saved to {task1_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f95859",
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1748026043020,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "07f95859"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import whisperx\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_output(model, eval_dataloader, version_dir):  \n",
    "    print(\"Calculating ouput score...\")\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "    export_json_file = {}\n",
    "\n",
    "    # 預先載入兩種語言的對齊模型\n",
    "    align_model_en, metadata_en = whisperx.load_align_model(language_code=\"en\", device=\"cuda\")\n",
    "    align_model_zh, metadata_zh = whisperx.load_align_model(language_code=\"zh\", device=\"cuda\")\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"處理評估資料\", unit=\"batch\"):\n",
    "        with torch.no_grad():\n",
    "            audio = batch[\"file_path\"]\n",
    "            file_id_str = batch[\"audio_file_name\"]\n",
    "            file_id_num = int(re.sub(r\"\\D\", \"\", file_id_str))\n",
    "            language_hint = \"zh\" if file_id_num >= 80000 else \"en\"\n",
    "\n",
    "            # 語音轉文字\n",
    "            result = model.transcribe(audio, batch_size=1, language=language_hint)\n",
    "\n",
    "            # 防止空結果崩潰\n",
    "            if len(result.get(\"segments\", [])) == 0:\n",
    "                print(f\"⚠️ 無轉錄結果: {file_id_str}\")\n",
    "                predictions.append(\"\")\n",
    "                filenames.append(file_id_str)\n",
    "                export_json_file[file_id_str] = {\n",
    "                    \"language\": language_hint,\n",
    "                    \"segments\": []\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            segments = result[\"segments\"]\n",
    "            transcript_dict = {\n",
    "                \"language\": result[\"language\"],\n",
    "                \"segments\": []\n",
    "            }\n",
    "\n",
    "            # 選擇對應語言的對齊模型\n",
    "            if language_hint == \"zh\":\n",
    "                alignment = whisperx.align(segments, align_model_zh, metadata_zh, audio, device=\"cuda\")\n",
    "            else:\n",
    "                alignment = whisperx.align(segments, align_model_en, metadata_en, audio, device=\"cuda\")\n",
    "\n",
    "            result[\"word_segments\"] = alignment[\"word_segments\"]\n",
    "            word_segments = alignment[\"word_segments\"]\n",
    "\n",
    "            for seg in segments:\n",
    "                words_in_seg = [\n",
    "                    {\n",
    "                        \"word\": w[\"word\"],\n",
    "                        \"start\": w[\"start\"],\n",
    "                        \"end\": w[\"end\"],\n",
    "                        \"probability\": w.get(\"probability\", None)\n",
    "                    }\n",
    "                    for w in word_segments\n",
    "                    if w[\"start\"] >= seg[\"start\"] and w[\"end\"] <= seg[\"end\"]\n",
    "                ]\n",
    "                transcript_dict[\"segments\"].append({\n",
    "                    \"text\": seg[\"text\"],\n",
    "                    \"start\": seg[\"start\"],\n",
    "                    \"end\": seg[\"end\"],\n",
    "                    \"words\": words_in_seg\n",
    "                })\n",
    "\n",
    "            pred_str = segments[0][\"text\"]\n",
    "            export_json_file[file_id_str] = transcript_dict\n",
    "            predictions.append(pred_str)\n",
    "            filenames.append(file_id_str)\n",
    "\n",
    "    save_predictions_to_tsv(predictions, filenames, version_dir, export_json_file)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91df8405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58853,
     "status": "ok",
     "timestamp": 1748026104308,
     "user": {
      "displayName": "方亮鈞",
      "userId": "10123484387827931003"
     },
     "user_tz": -480
    },
    "id": "91df8405",
    "outputId": "f47d5d18-eba8-4f74-cd5f-09e534568f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ouput score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/miniconda3/envs/lin/lib/python3.11/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "處理評估資料:   0%|          | 0/709 [00:00<?, ?batch/s]/home/student1/miniconda3/envs/lin/lib/python3.11/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "處理評估資料:  47%|████▋     | 334/709 [06:39<05:18,  1.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active speech found in audio\n",
      "⚠️ 無轉錄結果: 84296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理評估資料: 100%|██████████| 709/709 [14:14<00:00,  1.20s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions to file...\n",
      "Results saved to model_result/task1/v1/task1_answer.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(model, processed_dataset[\"val\"], version_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0988dd35b617477087e7395004b0b2fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29ee39fda62948d5809f7a27ccd6e10d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba43ffec427441fb9876820341c4c35",
      "placeholder": "​",
      "style": "IPY_MODEL_0988dd35b617477087e7395004b0b2fc",
      "value": " 20/20 [00:18&lt;00:00,  5.89 examples/s]"
     }
    },
    "382aa3bb596b4f7f9a7da828ae568b2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f118e5edfe74538b2332d4edcee7cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dda8b26af1f446f99a4c9f252883ffd6",
       "IPY_MODEL_7529fd6f27c946a2a206c1085d755b8f",
       "IPY_MODEL_29ee39fda62948d5809f7a27ccd6e10d"
      ],
      "layout": "IPY_MODEL_85a6a3231f2e4b4a82fcde55803978e2"
     }
    },
    "6ba43ffec427441fb9876820341c4c35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7529fd6f27c946a2a206c1085d755b8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0dd9f90f828471abdb5eefd244f64f5",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c99ddc0ee4ae4b20966ea8cdc0a54d0d",
      "value": 20
     }
    },
    "85a6a3231f2e4b4a82fcde55803978e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b96b87c377df49ecbe1f0e2e19090875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0dd9f90f828471abdb5eefd244f64f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c99ddc0ee4ae4b20966ea8cdc0a54d0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dda8b26af1f446f99a4c9f252883ffd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_382aa3bb596b4f7f9a7da828ae568b2e",
      "placeholder": "​",
      "style": "IPY_MODEL_b96b87c377df49ecbe1f0e2e19090875",
      "value": "Map: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
